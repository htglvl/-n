{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "758595b2-15ee-4d09-9374-74d7e7b14432",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install transformers\n",
    "# !pip install transformers[torch]\n",
    "# !pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f13672e-d441-498d-a487-096a6844d96e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # uncomment the following lines to install the necessary data from huggingface (wait till 1500 .hf folders install)\n",
    "# !sudo apt-get install git-lfs\n",
    "# !git lfs install \n",
    "# !git clone https://huggingface.co/datasets/tuan124816/newcs2_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a5acc51-72fc-4fa9-b8fe-d5cba65854f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from datasets import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from transformers import DecisionTransformerConfig, DecisionTransformerModel, Trainer, TrainingArguments\n",
    "import torch.nn.functional as F\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True  \n",
    "from PIL import Image\n",
    "from torchvision.models import efficientnet_b0\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "from torch.quantization import quantize_dynamic\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\" \n",
    "from datasets import disable_progress_bars\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "from functools import partial\n",
    "import shutil\n",
    "disable_progress_bars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7c2abcb-0339-4329-a883-035667367b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\setup_cac_kieu\\Anaconda\\New folder\\envs\\thesis\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "f:\\setup_cac_kieu\\Anaconda\\New folder\\envs\\thesis\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (8): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=True)\n",
       "    (1): DynamicQuantizedLinear(in_features=1280, out_features=1000, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define preprocessing steps for EfficientNet\n",
    "efficientnet_preprocessor = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.5], std=[0.5])  # Normalization\n",
    "])\n",
    "\n",
    "efficientnet = efficientnet_b0(pretrained=True).to(device)\n",
    "efficientnet = quantize_dynamic(efficientnet, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "efficientnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b99720eb-67c4-456c-99cf-a32f5df52b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_observations(example, offset_amount = 1, offset_column = 1):\n",
    "    # Roll the observations by the specified offset amount along the timestep dimension\n",
    "    if offset_column >= 1:\n",
    "        example['observations'] = torch.roll(torch.tensor(example['observations']).clone().detach(), shifts=offset_amount, dims=0).tolist()\n",
    "    if offset_column >= 2:\n",
    "        example['actions'] = torch.roll(torch.tensor(example['actions']).clone().detach(), shifts=offset_amount, dims=0).tolist()\n",
    "    if offset_column >= 3:\n",
    "        example['rewards'] = torch.roll(torch.tensor(example['rewards']).clone().detach(), shifts=offset_amount, dims=0).tolist()\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89d7c974-f769-4eee-b76e-f56922e293b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img(img):\n",
    "    # Convert 1D array to 2D if needed (e.g., 150x412 -> 224x224)\n",
    "    img = img.numpy().reshape(150, 412)\n",
    "    img_pil = Image.fromarray(img.astype(np.uint8))  # Convert to PIL image\n",
    "    \n",
    "    # Resize to 224x224 for EfficientNet\n",
    "    img_resized = img_pil.resize((224, 224))\n",
    "\n",
    "    # Convert grayscale (1 channel) to RGB (3 channels)\n",
    "    img_rgb = np.stack([np.array(img_resized)] * 3, axis=-1)\n",
    "    img_rgb = Image.fromarray(img_rgb)  # Convert back to PIL Image\n",
    "\n",
    "    # Apply EfficientNet preprocessing\n",
    "    img_tensor = efficientnet_preprocessor(img_rgb)\n",
    "    return img_tensor\n",
    "\n",
    "def extract_features_with_efficientnet(images, batch_size=8):\n",
    "    # Preprocess and extract features in smaller batches\n",
    "    pooled_features = []\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch_images = images[i:i + batch_size]\n",
    "        processed_images = torch.stack(batch_images).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = efficientnet.features(processed_images)  # Intermediate features\n",
    "            pooled_batch = torch.mean(outputs, dim=(2, 3))  # Global Average Pooling\n",
    "        pooled_features.append(pooled_batch)\n",
    "\n",
    "    return torch.cat(pooled_features, dim=0)\n",
    "\n",
    "\n",
    "def get_states_and_images(dataset):\n",
    "    states = []\n",
    "    images = []\n",
    "\n",
    "    for mini_dts in dataset.with_format(\"torch\"):\n",
    "        for state in mini_dts[\"observations\"]:\n",
    "            states.append(state[:3]) # This take the state part of the state\n",
    "            images.append(resize_img(state[3:])) # This take the image part of the state (1D image)\n",
    "\n",
    "    return states, images\n",
    "\n",
    "\n",
    "def get_new_state(dataset, model):\n",
    "    states_list, images_list = get_states_and_images(dataset)\n",
    "    states_features = torch.stack(states_list).to(device)\n",
    "    \n",
    "    # Extract image features with EfficientNet-B0\n",
    "    image_features = extract_features_with_efficientnet(images_list)\n",
    "\n",
    "    # Combine state and image features\n",
    "    combined_features = torch.cat((states_features, image_features), dim=1)\n",
    "    new_features = combined_features.reshape(20, 50, -1)\n",
    "\n",
    "    return new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8e4af0c-8358-4ce4-88c0-038c56ae5a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_observations(example, idx):\n",
    "    example['observations'] = new_obs_features[idx]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d65a84b2-b504-4a0d-8ad6-c1ecb334ed83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part if use load_from_disk\n",
    "import random\n",
    "def get_all_hf_folders(base_dir):\n",
    "    hf_folders = []\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for dir_name in dirs:\n",
    "            if dir_name.endswith('.hf'):\n",
    "                hf_folders.append(os.path.join(root, dir_name))\n",
    "    return hf_folders\n",
    "\n",
    "# base_dir = \"d:\\\\manual_data\"\n",
    "base_dir = \"newcs2_data\"\n",
    "# base_dir = \"d:\\\\newcs2_data_1\"\n",
    "hf_folder_paths = get_all_hf_folders(base_dir)\n",
    "random.shuffle(hf_folder_paths)\n",
    "def load_dataset_generator(hf_folder_paths): # pass in the list of arrow file paths\n",
    "    for folder in hf_folder_paths:\n",
    "        try: # Fall back to load_from_disk if fail and get DatasetGenerationError when using load_dataset\n",
    "            print(folder)\n",
    "            yield load_from_disk(folder).shuffle(seed=42)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load with 'load_from_disk'. Error: {e}\")\n",
    "\n",
    "dataset_stream = load_dataset_generator(hf_folder_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa353fa-e5e5-4882-a6be-28ea297bc899",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_dataset = next(iter(dataset_stream))\n",
    "first_dataset = first_dataset.with_format(\"torch\")\n",
    "new_obs_features = get_new_state(first_dataset, efficientnet)\n",
    "first_dataset = first_dataset.map(update_observations, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c946029-ca5b-4c9b-b289-05382edd1e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DecisionTransformerGymDataCollator:\n",
    "    return_tensors: str = \"pt\"\n",
    "    max_len: int = 50 #subsets of the episode we use for training\n",
    "    state_dim: int = 1283  # size of state space\n",
    "    act_dim: int = 51  # size of action space\n",
    "    max_ep_len: int = 1000 # max episode length in the dataset\n",
    "    scale: float = 1000.0  # normalization of rewards/returns\n",
    "    state_mean: np.array = None  # to store state means\n",
    "    state_std: np.array = None  # to store state stds\n",
    "    p_sample: np.array = None  # a distribution to take account trajectory lengths\n",
    "    n_traj: int = 0 # to store the number of trajectories in the dataset\n",
    "\n",
    "    def __init__(self, dataset) -> None:\n",
    "        self.act_dim = 51\n",
    "        self.state_dim = 1283\n",
    "        self.dataset = dataset.with_format('torch')\n",
    "        self.p_sample = np.array([0.05] * 20)\n",
    "\n",
    "    # def discount_cumsum(self, x, gamma): \n",
    "    #         return lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]\n",
    "\n",
    "    def discount_cumsum(self, x, gamma):\n",
    "        discount_cumsum = np.zeros_like(x)\n",
    "        discount_cumsum[-1] = x[-1]\n",
    "        for t in reversed(range(x.shape[0] - 1)):\n",
    "            discount_cumsum[t] = x[t] + gamma * discount_cumsum[t + 1]\n",
    "        return discount_cumsum\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # for i in range(50):\n",
    "        # self.dataset = self.dataset.map(offset_observations)\n",
    "        offset_amount = 2\n",
    "        offset_padding = random.randrange(0,30)\n",
    "        \n",
    "        offset_func = partial(offset_observations, offset_amount=offset_amount, offset_column = 1)\n",
    "        padding_func = partial(offset_observations, offset_amount=offset_padding, offset_column = 3)\n",
    "        self.dataset = self.dataset.map(offset_func)\n",
    "        self.dataset = self.dataset.map(padding_func)\n",
    "\n",
    "\n",
    "        mask =      np.concatenate((np.array([0] * (offset_amount+offset_padding)), np.array([1] * (self.max_len - offset_amount - offset_padding)  )   )  )\n",
    "        time_step = np.concatenate((np.array([0] * (offset_amount+offset_padding)), np.arange(0,    self.max_len - offset_amount - offset_padding   )   )  )\n",
    "        return {\n",
    "            \"states\": self.dataset.with_format(\"torch\")[\"observations\"].to(device),#.to(device), # og\n",
    "            \"actions\": self.dataset.with_format(\"torch\")['actions'].to(device),#.to(device), # og\n",
    "            \"rewards\": self.dataset.with_format(\"torch\")['rewards'].reshape(20,50,1).to(device),#.to(device),\n",
    "            \"returns_to_go\": torch.from_numpy(np.array(self.discount_cumsum(self.dataset[\"rewards\"], gamma=1.0)).reshape(20, 50, 1)).float(),#.to(device),\n",
    "            \"timesteps\":      torch.from_numpy(time_step).long().repeat(20, 1).to(device),#.to(device),\n",
    "            \"attention_mask\": torch.from_numpy(mask).float().repeat(20, 1).to(device),#.to(device),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c03f8447-fec3-4cde-9611-a8dd1a130657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Decision Transformer part\n",
    "class TrainableDT(DecisionTransformerModel):\n",
    "    def __init__(self, config, gamma=0.99):\n",
    "        super().__init__(config)\n",
    "        self.gamma = gamma\n",
    "        self.n_keys = 11\n",
    "        self.n_clicks = 2\n",
    "        mouse_x_possibles = [-1000.0,-500.0, -300.0, -200.0, -100.0, -60.0, -30.0, -20.0, -10.0, -4.0, -2.0, -0.0, 2.0, 4.0, 10.0, 20.0, 30.0, 60.0, 100.0, 200.0, 300.0, 500.0,1000.0]\n",
    "        mouse_y_possibles = [-200.0, -100.0, -50.0, -20.0, -10.0, -4.0, -2.0, -0.0, 2.0, 4.0, 10.0, 20.0, 50.0, 100.0, 200.0]\n",
    "        self.n_mouse_x = len(mouse_x_possibles)\n",
    "        self.n_mouse_y = len(mouse_y_possibles)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        global model_output, predict_value, targeter_value\n",
    "        # Reshape for custom loss\n",
    "\n",
    "        output = super().forward(**kwargs)\n",
    "        model_output = output\n",
    "        action_preds = output[1] \n",
    "        action_targets = kwargs[\"actions\"]\n",
    "        attention_mask = kwargs[\"attention_mask\"]\n",
    "        act_dim = action_preds.shape[2]\n",
    "        action_preds = action_preds.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "        predict_value = action_preds\n",
    "        action_targets = action_targets.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "        targeter_value = action_targets\n",
    "\n",
    "        # print('|'*50)\n",
    "        # print(\"action_preds\", action_preds)\n",
    "        # print(\"action_targets\", action_targets)\n",
    "        # print('|'*50)\n",
    "        \n",
    "        # Loss calculations based on each category\n",
    "        loss_wasd = F.binary_cross_entropy_with_logits(action_preds[:, :4], action_targets[:, :4])\n",
    "        loss_space = F.binary_cross_entropy_with_logits(action_preds[:, 4:5], action_targets[:, 4:5])\n",
    "        loss_weapon_switch = F.binary_cross_entropy_with_logits(action_preds[:, self.n_keys-4:self.n_keys-1], action_targets[:, self.n_keys-4:self.n_keys-1])\n",
    "        loss_reload = F.binary_cross_entropy_with_logits(action_preds[:, self.n_keys-1:self.n_keys], action_targets[:, self.n_keys-1:self.n_keys])\n",
    "        loss_left_click = F.binary_cross_entropy_with_logits(action_preds[:, self.n_keys:self.n_keys+1], action_targets[:, self.n_keys:self.n_keys+1])\n",
    "        loss_right_click = F.binary_cross_entropy_with_logits(action_preds[:, self.n_keys+1:self.n_keys+self.n_clicks], action_targets[:, self.n_keys+1:self.n_keys+self.n_clicks])\n",
    "        loss_mouse_move_x = F.cross_entropy(action_preds[:, self.n_keys+self.n_clicks:self.n_keys+self.n_clicks+self.n_mouse_x], action_targets[:, self.n_keys+self.n_clicks:self.n_keys+self.n_clicks+self.n_mouse_x])\n",
    "        loss_mouse_move_y = F.cross_entropy(action_preds[:, self.n_keys+self.n_clicks+self.n_mouse_x:self.n_keys+self.n_clicks+self.n_mouse_x+self.n_mouse_y], action_targets[:, self.n_keys+self.n_clicks+self.n_mouse_x:self.n_keys+self.n_clicks+self.n_mouse_x+self.n_mouse_y])\n",
    "\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = sum([loss_wasd,loss_space, loss_weapon_switch, loss_reload, loss_left_click, loss_right_click, loss_mouse_move_x, loss_mouse_move_y])\n",
    "        print(\"+\"*50)\n",
    "        print(\"loss_wasd\", loss_wasd, end='|||')\n",
    "        print(\"loss_left_click\", loss_left_click)\n",
    "        print(\"loss_mouse_move_x\", loss_mouse_move_x, end='|||')\n",
    "        print(\"loss_mouse_move_y\", loss_mouse_move_y)\n",
    "        print(\"total loss\", total_loss)\n",
    "        print(\"+\"*50)\n",
    "        return {\"loss\": total_loss}\n",
    "\n",
    "    def original_forward(self, **kwargs):\n",
    "        return super().forward(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ec58bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"trained_models_2/\",\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=25,\n",
    "    per_device_train_batch_size=1, \n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.0001,\n",
    "    optim = \"adamw_torch_fused\",\n",
    "    # lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm=1.0,\n",
    "    dataloader_pin_memory = False,\n",
    "    tf32=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08b6489-0d62-4d4b-ada7-3b6b8a55bdae",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "collator = DecisionTransformerGymDataCollator(first_dataset)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=first_dataset,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "# train the first dataset\n",
    "trainer.train()\n",
    "save_path = \"trained_models_2/model_after_dataset_1_in_loops_0\"\n",
    "trainer.save_model(save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff6d503-1357-4e1e-8c7c-c52dc0a30071",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for loops in range(100):\n",
    "    start_num = 53 # custome theo số cuối của model trước + 1\n",
    "    for i, dataset in enumerate(dataset_stream, start=start_num):\n",
    "        dataset = dataset.with_format(\"torch\")\n",
    "        new_obs_features = get_new_state(dataset, efficientnet)\n",
    "        dataset = dataset.map(update_observations, with_indices=True)\n",
    "        collator = DecisionTransformerGymDataCollator(dataset)\n",
    "        print(f\"USE ++ model_after_dataset_{i-1} ++ FOR TRAINING NOW\")\n",
    "        # trained_models_2/model_after_dataset_1_in_loops_0\n",
    "        model = TrainableDT.from_pretrained(f\"trained_models_2/model_after_dataset_{i-1}_in_loops_{loops}\").to(device)\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset,\n",
    "            data_collator=collator,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        save_path = f\"trained_models_2/model_after_dataset_{i}_in_loops_{loops}\"\n",
    "        trainer.save_model(save_path)\n",
    "        if os.path.exists(f\"trained_models_2/model_after_dataset_{i-2}_in_loops_{loops}\") and i-2 % 10 == 0:\n",
    "            shutil.rmtree(f\"trained_models_2/model_after_dataset_{i-2}_in_loops_{loops}\")\n",
    "        print(f\"Model saved to {save_path}\")\n",
    "        \n",
    "    dataset_stream = load_dataset_generator(hf_folder_paths)\n",
    "    first_dataset = next(dataset_stream).with_format('torch')\n",
    "    new_obs_features = get_new_state(first_dataset, efficientnet)\n",
    "    dataset = first_dataset.map(update_observations, with_indices=True)\n",
    "\n",
    "    collator = DecisionTransformerGymDataCollator(dataset)\n",
    "    print(f\"USE ++ model_after_dataset_{i-1} ++ FOR TRAINING NOW\")\n",
    "\n",
    "    model = TrainableDT.from_pretrained(f\"trained_models_2/model_after_dataset_{len(hf_folder_paths)-1}_in_loops_{loops}\").to(device)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "    trainer.train()\n",
    "    save_path = f\"trained_models_2/model_after_dataset_1_in_loops_{loops+1}\"\n",
    "    trainer.save_model(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80630410-6ed9-44fc-99f8-e7dc6dc2cd49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01479543-3006-4fbe-98ef-a9d215999807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
