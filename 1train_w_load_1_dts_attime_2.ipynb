{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "import copy\n",
    "import h5py\n",
    "from datasets import Dataset, Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, concatenate_datasets, load_from_disk\n",
    "from transformers import ViTFeatureExtractor, ViTModel\n",
    "from transformers import DecisionTransformerConfig, DecisionTransformerModel, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm \n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "# Testing multi CPU for loading data\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_from_disk(\"f:\\hdf5_dm_test_full_hoang.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\setup_cac_kieu\\Anaconda\\New folder\\envs\\thesis\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "vit_model = ViTModel.from_pretrained('google/vit-base-patch16-224').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DecisionTransformerGymDataCollator:\n",
    "    return_tensors: str = \"pt\"\n",
    "    max_len: int = 20 #subsets of the episode we use for training\n",
    "    state_dim: int = 61803  # size of state space\n",
    "    act_dim: int = 51  # size of action space\n",
    "    max_ep_len: int = 1000 # max episode length in the dataset\n",
    "    scale: float = 1000.0 # normalization of rewards/returns\n",
    "    state_mean: np.array = None  # to store state means\n",
    "    state_std: np.array = None  # to store state stds\n",
    "    p_sample: np.array = None  # a distribution to take account trajectory lengths\n",
    "    n_traj: int = 0 # to store the number of trajectories in the dataset\n",
    "\n",
    "    def __init__(self, dataset) -> None:\n",
    "        self.act_dim = len(dataset[0][\"actions\"][0])\n",
    "        self.state_dim = len(dataset[0][\"observations\"][0])\n",
    "        \n",
    "        \n",
    "        self.dataset = dataset\n",
    "        # calculate dataset stats for normalization of states\n",
    "        states = []\n",
    "        traj_lens = []\n",
    "        for obs in dataset[\"observations\"]:\n",
    "            states.extend(obs)\n",
    "            traj_lens.append(len(obs))\n",
    "        self.n_traj = len(traj_lens)\n",
    "        states = np.vstack(states)\n",
    "        self.state_mean, self.state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "\n",
    "        traj_lens = np.array(traj_lens)\n",
    "        self.p_sample = traj_lens / sum(traj_lens)\n",
    "\n",
    "    def _discount_cumsum(self, x, gamma):\n",
    "        discount_cumsum = np.zeros_like(x)\n",
    "        discount_cumsum[-1] = x[-1]\n",
    "        for t in reversed(range(x.shape[0] - 1)):\n",
    "            discount_cumsum[t] = x[t] + gamma * discount_cumsum[t + 1]\n",
    "        return discount_cumsum\n",
    "\n",
    "    def __call__(self, features):\n",
    "        batch_size = len(features)\n",
    "        # this is a bit of a hack to be able to sample of a non-uniform distribution\n",
    "        batch_inds = np.random.choice(\n",
    "            np.arange(self.n_traj),\n",
    "            size=batch_size,\n",
    "            replace=True,\n",
    "            p=self.p_sample,  # reweights so we sample according to timesteps\n",
    "        )\n",
    "        # a batch of dataset features\n",
    "        s, a, r, rtg, timesteps, mask, image_features = [], [], [], [], [], [], []\n",
    "\n",
    "        for ind in batch_inds:\n",
    "            # for feature in features:\n",
    "            feature = self.dataset[int(ind)]\n",
    "            si = random.randint(0, len(feature[\"rewards\"]) - 1) #random starting index for each selected trajectory.\n",
    "\n",
    "            # get sequences from dataset\n",
    "            s.append(np.array(feature[\"observations\"][si : si + self.max_len]).reshape(1, -1, self.state_dim)) # take max_len time frames from starting index(si)\n",
    "            a.append(np.array(feature[\"actions\"][si : si + self.max_len]).reshape(1, -1, self.act_dim))\n",
    "            r.append(np.array(feature[\"rewards\"][si : si + self.max_len]).reshape(1, -1, 1))\n",
    "            \n",
    "            # Extract image from the observations key then reshape to 150,412- then append to a list like all others data, hope so\n",
    "            images = [np.array(img[3:]).reshape(150,412) for img in feature[\"observations\"][si : si + self.max_len]]\n",
    "            images = [np.repeat(img[:, :, np.newaxis], 3, axis=2) for img in images]  # Need to do this because ViT expects 3 channels but the images are 2D grayscale\n",
    "            images = [(img / 255.0).astype(np.float32) for img in images]  # Normalize the images to [0, 1] to avoide errors ValueError: The image to be converted to a PIL image contains values outside the range [0, 1], got [9.666666666666666, 227.33333333333334] which cannot be converted to uint8.\"}\n",
    "            with torch.no_grad():\n",
    "                inputs = feature_extractor(images, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "                vit_outputs = vit_model(**inputs)\n",
    "                image_features.append(vit_outputs.last_hidden_state.mean(dim=1).cpu().numpy().reshape(1, -1, vit_model.config.hidden_size)) # need to get these thing to cpu to convert back to numpy, fitting other data\n",
    "                \n",
    "            # d.append(np.array(feature[\"dones\"][si : si + self.max_len]).reshape(1, -1))\n",
    "            timesteps.append(np.arange(si, si + s[-1].shape[1]).reshape(1, -1))\n",
    "            timesteps[-1][timesteps[-1] >= self.max_ep_len] = self.max_ep_len - 1  # padding cutoff\n",
    "            rtg.append(\n",
    "                self._discount_cumsum(np.array(feature[\"rewards\"][si:]), gamma=1.0)[\n",
    "                    : s[-1].shape[1]   # TODO check the +1 removed here\n",
    "                ].reshape(1, -1, 1)\n",
    "            )\n",
    "            if rtg[-1].shape[1] < s[-1].shape[1]:\n",
    "                print(\"if true\")\n",
    "                rtg[-1] = np.concatenate([rtg[-1], np.zeros((1, 1, 1))], axis=1)\n",
    "\n",
    "            # padding and state + reward normalization\n",
    "            tlen = s[-1].shape[1]\n",
    "            s[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, self.state_dim)), s[-1]], axis=1)\n",
    "            s[-1] = (s[-1] - self.state_mean) / self.state_std\n",
    "            a[-1] = np.concatenate(\n",
    "                [np.ones((1, self.max_len - tlen, self.act_dim)) * -10.0, a[-1]],\n",
    "                axis=1,\n",
    "            )\n",
    "            r[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), r[-1]], axis=1)\n",
    "            rtg[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), rtg[-1]], axis=1) / self.scale\n",
    "            timesteps[-1] = np.concatenate([np.zeros((1, self.max_len - tlen)), timesteps[-1]], axis=1)\n",
    "            mask.append(np.concatenate([np.zeros((1, self.max_len - tlen)), np.ones((1, tlen))], axis=1))\n",
    "\n",
    "        # For padding like above, avoid error ValueError: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 10 and the array at index 1 has size 20\n",
    "        max_size = max(arr.shape[1] for arr in image_features)\n",
    "        # padded_image_features = [np.pad(arr, ((0, 0), (0, max_size - arr.shape[1])), mode='constant') for arr in image_features]\n",
    "        padded_image_features = []\n",
    "        for arr in image_features:\n",
    "            # Ensure `arr` has the correct number of dimensions, typically (1, max_len, hidden_size)\n",
    "            if arr.shape[1] < max_size:\n",
    "                padding_shape = ((0, 0), (0, max_size - arr.shape[1]), (0, 0))\n",
    "                padded_arr = np.pad(arr, padding_shape, mode='constant')\n",
    "            else:\n",
    "                padded_arr = arr\n",
    "            padded_image_features.append(padded_arr)\n",
    "\n",
    "        s = torch.from_numpy(np.concatenate(s, axis=0)).float()\n",
    "        a = torch.from_numpy(np.concatenate(a, axis=0)).float()\n",
    "        r = torch.from_numpy(np.concatenate(r, axis=0)).float()\n",
    "        rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).float()\n",
    "        timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).long()\n",
    "        mask = torch.from_numpy(np.concatenate(mask, axis=0)).float()\n",
    "        images = torch.from_numpy(np.concatenate(padded_image_features, axis=0)).float()\n",
    "        \n",
    "\n",
    "\n",
    "        return {\n",
    "            \"states\": s,\n",
    "            \"actions\": a,\n",
    "            \"rewards\": r,\n",
    "            \"returns_to_go\": rtg,\n",
    "            \"timesteps\": timesteps,\n",
    "            \"attention_mask\": mask,\n",
    "            \"image_features\": images,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainableDT(DecisionTransformerModel):\n",
    "    def __init__(self, config, image_feature_dim=768, gamma=0.99):\n",
    "        super().__init__(config)\n",
    "        self.gamma = gamma\n",
    "        self.image_feature_dim = image_feature_dim\n",
    "        self.state_projector = nn.Linear(config.state_dim + image_feature_dim, config.hidden_size)\n",
    "        self._states_projected = None\n",
    "\n",
    "    def embed_state(self, states):\n",
    "        # Use the projected states instead of embedding again\n",
    "        if self._states_projected is not None:\n",
    "            return self._states_projected\n",
    "        # Fallback to the default embed_state behavior if needed\n",
    "        return super().embed_state(states)\n",
    "\n",
    "    def forward(self, **kwargs): # kwarg need image features\n",
    "        # Reshape for custom loss\n",
    "        n_keys = 11\n",
    "        n_clicks = 2\n",
    "        mouse_x_possibles = [-1000.0,-500.0, -300.0, -200.0, -100.0, -60.0, -30.0, -20.0, -10.0, -4.0, -2.0, -0.0, 2.0, 4.0, 10.0, 20.0, 30.0, 60.0, 100.0, 200.0, 300.0, 500.0,1000.0]\n",
    "        mouse_y_possibles = [-200.0, -100.0, -50.0, -20.0, -10.0, -4.0, -2.0, -0.0, 2.0, 4.0, 10.0, 20.0, 50.0, 100.0, 200.0]\n",
    "        n_mouse_x = len(mouse_x_possibles)\n",
    "        n_mouse_y = len(mouse_y_possibles)\n",
    "        \n",
    "        # For image and state features\n",
    "        states = kwargs.pop(\"states\")\n",
    "        image_features = kwargs.pop(\"image_features\")\n",
    "        states_with_images = torch.cat([states, image_features], dim=-1)\n",
    "        # states_projected = self.state_projector(states_with_images)\n",
    "        self._states_projected = self.state_projector(states_with_images)\n",
    "        kwargs[\"states\"] = self._states_projected\n",
    "        output = super().forward(**kwargs)\n",
    "        self._states_projected = None\n",
    "        \n",
    "        action_preds = output[1]\n",
    "        action_targets = kwargs[\"actions\"]\n",
    "        attention_mask = kwargs[\"attention_mask\"]\n",
    "        act_dim = action_preds.shape[2]\n",
    "        action_preds = action_preds.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "        action_targets = action_targets.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "\n",
    "\n",
    "        # Action_pred n_keys: 11 keys for wasd, space, reload, 1,2,3\n",
    "        action_preds[:, :n_keys] = torch.sigmoid(action_preds[:, :n_keys])\n",
    "        # Action_pred n_clicks: left click, right click\n",
    "        action_preds[:, n_keys:n_keys+n_clicks] = torch.sigmoid(action_preds[:, n_keys:n_keys+n_clicks])\n",
    "        # # Action_pred n_mouse_x: to the x mouse poll\n",
    "        # action_preds[:, n_keys + n_clicks:n_keys + n_clicks + n_mouse_x] = F.softmax(action_preds[:, n_keys + n_clicks:n_keys + n_clicks + n_mouse_x], dim=-1)\n",
    "        # # Action_pred n_mouse_y: to the y mouse poll\n",
    "        # action_preds[:, n_keys + n_clicks + n_mouse_x:n_keys + n_clicks + n_mouse_x + n_mouse_y] = F.softmax(action_preds[:, n_keys + n_clicks + n_mouse_x:n_keys + n_clicks + n_mouse_x + n_mouse_y], dim=-1)\n",
    "\n",
    "        # Loss calculations based on each category\n",
    "        loss_wasd = F.binary_cross_entropy(action_preds[:, :4], action_targets[:, :4])\n",
    "        loss_left_click = F.binary_cross_entropy(action_preds[:, n_keys:n_keys+1], action_targets[:, n_keys:n_keys+1])\n",
    "        loss_mouse_move_x = F.cross_entropy(action_preds[:, n_keys+n_clicks:n_keys+n_clicks+n_mouse_x], action_targets[:, n_keys+n_clicks:n_keys+n_clicks+n_mouse_x])\n",
    "        loss_mouse_move_y = F.cross_entropy(action_preds[:, n_keys+n_clicks+n_mouse_x:n_keys+n_clicks+n_mouse_x+n_mouse_y], action_targets[:, n_keys+n_clicks+n_mouse_x:n_keys+n_clicks+n_mouse_x+n_mouse_y])\n",
    "\n",
    "        # loss_crit = 10 * F.mse_loss(reward_t[:, :-1] + self.gamma * v_t_next, v_t[:, :-1])\n",
    "        # Total loss\n",
    "        # total_loss = sum([3*loss_wasd,0.25*loss_space, 0.25*loss_reload,0.25*loss_weapon_switch, 2*loss_left_click, 0.25*loss_right_click, 2*loss_mouse_move_x,  2*loss_mouse_move_y])\n",
    "        total_loss = sum([loss_wasd, loss_left_click, loss_mouse_move_x, loss_mouse_move_y])\n",
    "        print(\"|\"*100)\n",
    "        print(\"Loss WASD\",loss_wasd)\n",
    "        print(\"Loss lclick\",loss_left_click)\n",
    "        print(\"Loss mx\",loss_mouse_move_x)\n",
    "        print(\"Loss my\",loss_mouse_move_y)\n",
    "        print(\"loss\", total_loss)\n",
    "        print(\"|\"*100)\n",
    "        print(\"loss\", total_loss)\n",
    "        return {\"loss\": total_loss}\n",
    "\n",
    "    def original_forward(self, **kwargs):\n",
    "        return super().forward(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom generator for streaming datasets\n",
    "def load_dataset_generator():\n",
    "    for folder in hf_folders:\n",
    "        yield load_from_disk(folder)\n",
    "\n",
    "\n",
    "main_data_path = \"d:\\\\test\"\n",
    "# Get all .hf folder paths\n",
    "hf_folders = [os.path.join(main_data_path, folder) for folder in os.listdir(main_data_path) if folder.endswith(\".hf\")]\n",
    "\n",
    "# Load dataset generator\n",
    "dataset_stream = load_dataset_generator()\n",
    "# for data in dataset_stream:\n",
    "#     print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainableDT(\n",
      "  (encoder): DecisionTransformerGPT2Model(\n",
      "    (wte): Embedding(1, 1024)\n",
      "    (wpe): Embedding(1024, 1024)\n",
      "    (drop): Dropout(p=0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-19): 20 x DecisionTransformerGPT2Block(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): DecisionTransformerGPT2Attention(\n",
      "          (c_attn): Conv1D(nf=3072, nx=1024)\n",
      "          (c_proj): Conv1D(nf=1024, nx=1024)\n",
      "          (attn_dropout): Dropout(p=0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): DecisionTransformerGPT2MLP(\n",
      "          (c_fc): Conv1D(nf=4096, nx=1024)\n",
      "          (c_proj): Conv1D(nf=1024, nx=4096)\n",
      "          (act): ReLU()\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (embed_timestep): Embedding(4096, 1024)\n",
      "  (embed_return): Linear(in_features=1, out_features=1024, bias=True)\n",
      "  (embed_state): Linear(in_features=61803, out_features=1024, bias=True)\n",
      "  (embed_action): Linear(in_features=51, out_features=1024, bias=True)\n",
      "  (embed_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (predict_state): Linear(in_features=1024, out_features=61803, bias=True)\n",
      "  (predict_action): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=51, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      "  (predict_return): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  (state_projector): Linear(in_features=62571, out_features=1024, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/trained_models/\",\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=32,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=1e-4,\n",
    "    warmup_ratio=0.3,\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    # max_grad_norm=0.5,\n",
    ")\n",
    "\n",
    "# Initialize model and config once, outside the loop\n",
    "# Load the first dataset to get dimensions for the initial model configuration\n",
    "config = DecisionTransformerConfig(state_dim=61803, act_dim=51, hidden_size=1024, n_layer=20, n_head=16, dropout=0.0)\n",
    "first_dataset = next(dataset_stream)\n",
    "collator = DecisionTransformerGymDataCollator(first_dataset)\n",
    "model = TrainableDT(config)\n",
    "print(model)\n",
    "# First training session\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=first_dataset,\n",
    "    data_collator=collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3777cafdb42402994255ab3f148f628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.7216, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.7296, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(3.2812, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.7995, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(7.5318, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(7.5318, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.7462, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.6531, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(3.2453, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.7693, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(7.4139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(7.4139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.6749, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.6589, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(3.0627, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.3058, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(6.7023, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(6.7023, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5655, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3310, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.9829, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.1177, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(5.9971, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(5.9971, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5162, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3950, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.8151, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.0094, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(5.7357, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(5.7357, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5365, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3133, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(3.0465, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.1805, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(6.0768, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(6.0768, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5847, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3197, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(3.0676, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.2056, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(6.1776, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(6.1776, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5503, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3201, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(3.0253, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(1.9851, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(5.8807, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(5.8807, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5604, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3133, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.9001, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.2543, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(6.0281, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(6.0281, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5571, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3195, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(3.0956, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.2444, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(6.2167, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(6.2167, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5287, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3234, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.9815, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.0644, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(5.8980, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(5.8980, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5679, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3133, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(3.1879, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.1594, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(6.2284, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(6.2284, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5554, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3133, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.8660, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.0080, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(5.7427, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(5.7427, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.4939, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3953, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.7969, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.0265, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(5.7125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(5.7125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5454, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3243, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.9508, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.1603, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(5.9809, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(5.9809, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5806, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3340, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(3.0013, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.1268, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(6.0427, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(6.0427, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5613, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3133, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.9927, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.2708, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(6.1381, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(6.1381, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5541, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3818, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.9354, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.0964, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(5.9677, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(5.9677, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5202, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3133, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.9132, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.1232, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(5.8699, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(5.8699, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5863, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3616, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(3.0047, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.3078, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(6.2604, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(6.2604, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5772, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3516, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(3.0202, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(1.9665, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(5.9154, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(5.9154, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5622, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3193, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(3.0035, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(1.9820, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(5.8670, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(5.8670, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5694, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3133, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.9792, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(1.8596, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(5.7214, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(5.7214, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5760, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3379, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.9566, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(1.9038, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(5.7743, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(5.7743, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5650, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3515, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.9299, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(1.7284, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(5.5748, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(5.5748, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5570, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3664, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.8940, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.1300, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(5.9473, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(5.9473, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5690, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3221, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.9762, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.1157, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(5.9830, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(5.9830, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5486, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3133, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(3.0294, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.2669, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(6.1582, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(6.1582, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5643, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3133, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.9113, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.2537, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(6.0425, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(6.0425, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5903, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3370, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.9949, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.2078, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(6.1301, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(6.1301, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5539, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3472, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.9324, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.3633, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(6.1969, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(6.1969, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5633, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3284, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.9522, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.3060, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(6.1499, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(6.1499, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5572, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3253, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.9220, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.2085, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(6.0131, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(6.0131, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5226, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3208, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.8516, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.0267, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(5.7217, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(5.7217, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5913, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3273, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.8370, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.1613, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(5.9169, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(5.9169, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5573, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3133, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.8768, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.0332, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(5.7805, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(5.7805, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5832, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3193, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.9078, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.3030, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(6.1134, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(6.1134, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5723, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3358, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.9342, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.0194, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(5.8618, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(5.8618, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5705, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3680, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.8691, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(1.8522, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(5.6598, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(5.6598, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5634, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3322, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.8877, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.0849, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(5.8683, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(5.8683, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5721, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3516, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.8227, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.0561, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(5.8025, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(5.8025, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5284, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.4660, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.7541, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.1997, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(5.9482, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(5.9482, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5790, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3133, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.8497, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.2794, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(6.0213, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(6.0213, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5435, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.4307, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.6868, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(1.9914, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(5.6524, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(5.6524, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5259, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3374, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.8407, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(1.8534, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(5.5574, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(5.5574, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5318, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3744, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.7088, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.2134, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(5.8283, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(5.8283, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.6067, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3133, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.8921, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.3249, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(6.1370, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(6.1370, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5639, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3738, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.8121, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.0708, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(5.8206, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(5.8206, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5946, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3224, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(3.0031, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.0153, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(5.9354, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(5.9354, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Loss WASD tensor(0.5355, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss lclick tensor(0.3190, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Loss mx tensor(2.8709, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "Loss my tensor(2.0558, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "loss tensor(5.7813, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "loss tensor(5.7813, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "{'train_runtime': 3733.0489, 'train_samples_per_second': 0.268, 'train_steps_per_second': 0.013, 'train_loss': 6.009659423828125, 'epoch': 50.0}\n",
      "Model saved to ./trained_models/model_after_dataset_1\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "# Save the initial model checkpoint\n",
    "save_path = \"./trained_models/model_after_dataset_1\"\n",
    "trainer.save_model(save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training with the remaining datasets\n",
    "for i, dataset in enumerate(dataset_stream, start=2):\n",
    "    # Update collator with the new dataset\n",
    "    collator = DecisionTransformerGymDataCollator(dataset)\n",
    "    \n",
    "    # Load the latest model checkpoint\n",
    "    print(f\"Use || model_after_dataset_{i-1} || for training now\")\n",
    "    model = TrainableDT.from_pretrained(f\"F:\\!Theis\\Final_prj\\\\trained_models\\model_after_dataset_{i-1}\")\n",
    "\n",
    "    # Set up the trainer with updated model and new dataset\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        data_collator=collator\n",
    "    )\n",
    "\n",
    "    # Train on the current dataset\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the model after each dataset\n",
    "    save_path = f\"./trained_models/model_after_dataset_{i}\"\n",
    "    trainer.save_model(save_path)\n",
    "    print(f\"Model saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
