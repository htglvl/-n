{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "import copy\n",
    "import h5py\n",
    "from datasets import Dataset, Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, concatenate_datasets, load_from_disk\n",
    "from transformers import ViTFeatureExtractor, ViTModel\n",
    "from transformers import DecisionTransformerConfig, DecisionTransformerModel, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm \n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "# Testing multi CPU for loading data\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_from_disk(\"f:\\hdf5_dm_test_full_hoang.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\setup_cac_kieu\\Anaconda\\New folder\\envs\\thesis\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "vit_model = ViTModel.from_pretrained('google/vit-base-patch16-224').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DecisionTransformerGymDataCollator:\n",
    "    return_tensors: str = \"pt\"\n",
    "    max_len: int = 20 #subsets of the episode we use for training\n",
    "    state_dim: int = 61803  # size of state space\n",
    "    act_dim: int = 51  # size of action space\n",
    "    max_ep_len: int = 1000 # max episode length in the dataset\n",
    "    scale: float = 1000.0  # normalization of rewards/returns\n",
    "    state_mean: np.array = None  # to store state means\n",
    "    state_std: np.array = None  # to store state stds\n",
    "    p_sample: np.array = None  # a distribution to take account trajectory lengths\n",
    "    n_traj: int = 0 # to store the number of trajectories in the dataset\n",
    "\n",
    "    def __init__(self, dataset) -> None:\n",
    "        self.act_dim = len(dataset[0][\"actions\"][0])\n",
    "        self.state_dim = len(dataset[0][\"observations\"][0])\n",
    "        \n",
    "        \n",
    "        self.dataset = dataset\n",
    "        # calculate dataset stats for normalization of states\n",
    "        states = []\n",
    "        traj_lens = []\n",
    "        for obs in dataset[\"observations\"]:\n",
    "            states.extend(obs)\n",
    "            traj_lens.append(len(obs))\n",
    "        self.n_traj = len(traj_lens)\n",
    "        states = np.vstack(states)\n",
    "        self.state_mean, self.state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "\n",
    "        traj_lens = np.array(traj_lens)\n",
    "        self.p_sample = traj_lens / sum(traj_lens)\n",
    "\n",
    "    def _discount_cumsum(self, x, gamma):\n",
    "        discount_cumsum = np.zeros_like(x)\n",
    "        discount_cumsum[-1] = x[-1]\n",
    "        for t in reversed(range(x.shape[0] - 1)):\n",
    "            discount_cumsum[t] = x[t] + gamma * discount_cumsum[t + 1]\n",
    "        return discount_cumsum\n",
    "\n",
    "    def __call__(self, features):\n",
    "        batch_size = len(features)\n",
    "        # this is a bit of a hack to be able to sample of a non-uniform distribution\n",
    "        batch_inds = np.random.choice(\n",
    "            np.arange(self.n_traj),\n",
    "            size=batch_size,\n",
    "            replace=True,\n",
    "            p=self.p_sample,  # reweights so we sample according to timesteps\n",
    "        )\n",
    "        # a batch of dataset features\n",
    "        s, a, r, rtg, timesteps, mask, image_features = [], [], [], [], [], [], []\n",
    "\n",
    "        for ind in batch_inds:\n",
    "            # for feature in features:\n",
    "            feature = self.dataset[int(ind)]\n",
    "            si = random.randint(0, len(feature[\"rewards\"]) - 1) #random starting index for each selected trajectory.\n",
    "\n",
    "            # get sequences from dataset\n",
    "            s.append(np.array(feature[\"observations\"][si : si + self.max_len]).reshape(1, -1, self.state_dim)) # take max_len time frames from starting index(si)\n",
    "            a.append(np.array(feature[\"actions\"][si : si + self.max_len]).reshape(1, -1, self.act_dim))\n",
    "            r.append(np.array(feature[\"rewards\"][si : si + self.max_len]).reshape(1, -1, 1))\n",
    "            \n",
    "            # Extract image from the observations key then reshape to 150,412- then append to a list like all others data, hope so\n",
    "            images = [np.array(img[3:]).reshape(150,412) for img in feature[\"observations\"][si : si + self.max_len]]\n",
    "            images = [np.repeat(img[:, :, np.newaxis], 3, axis=2) for img in images]  # Need to do this because ViT expects 3 channels but the images are 2D grayscale\n",
    "            images = [(img / 255.0).astype(np.float32) for img in images]  # Normalize the images to [0, 1] to avoide errors ValueError: The image to be converted to a PIL image contains values outside the range [0, 1], got [9.666666666666666, 227.33333333333334] which cannot be converted to uint8.\"}\n",
    "            with torch.no_grad():\n",
    "                inputs = feature_extractor(images, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "                vit_outputs = vit_model(**inputs)\n",
    "                image_features.append(vit_outputs.last_hidden_state.mean(dim=1).cpu().numpy().reshape(1, -1, vit_model.config.hidden_size)) # need to get these thing to cpu to convert back to numpy, fitting other data\n",
    "                \n",
    "            # d.append(np.array(feature[\"dones\"][si : si + self.max_len]).reshape(1, -1))\n",
    "            timesteps.append(np.arange(si, si + s[-1].shape[1]).reshape(1, -1))\n",
    "            timesteps[-1][timesteps[-1] >= self.max_ep_len] = self.max_ep_len - 1  # padding cutoff\n",
    "            rtg.append(\n",
    "                self._discount_cumsum(np.array(feature[\"rewards\"][si:]), gamma=1.0)[\n",
    "                    : s[-1].shape[1]   # TODO check the +1 removed here\n",
    "                ].reshape(1, -1, 1)\n",
    "            )\n",
    "            if rtg[-1].shape[1] < s[-1].shape[1]:\n",
    "                print(\"if true\")\n",
    "                rtg[-1] = np.concatenate([rtg[-1], np.zeros((1, 1, 1))], axis=1)\n",
    "\n",
    "            # padding and state + reward normalization\n",
    "            tlen = s[-1].shape[1]\n",
    "            s[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, self.state_dim)), s[-1]], axis=1)\n",
    "            s[-1] = (s[-1] - self.state_mean) / self.state_std\n",
    "            a[-1] = np.concatenate(\n",
    "                [np.ones((1, self.max_len - tlen, self.act_dim)) * -10.0, a[-1]],\n",
    "                axis=1,\n",
    "            )\n",
    "            r[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), r[-1]], axis=1)\n",
    "            rtg[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), rtg[-1]], axis=1) / self.scale\n",
    "            timesteps[-1] = np.concatenate([np.zeros((1, self.max_len - tlen)), timesteps[-1]], axis=1)\n",
    "            mask.append(np.concatenate([np.zeros((1, self.max_len - tlen)), np.ones((1, tlen))], axis=1))\n",
    "\n",
    "        # For padding like above, avoid error ValueError: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 10 and the array at index 1 has size 20\n",
    "        max_size = max(arr.shape[1] for arr in image_features)\n",
    "        # padded_image_features = [np.pad(arr, ((0, 0), (0, max_size - arr.shape[1])), mode='constant') for arr in image_features]\n",
    "        padded_image_features = []\n",
    "        for arr in image_features:\n",
    "            # Ensure `arr` has the correct number of dimensions, typically (1, max_len, hidden_size)\n",
    "            if arr.shape[1] < max_size:\n",
    "                padding_shape = ((0, 0), (0, max_size - arr.shape[1]), (0, 0))\n",
    "                padded_arr = np.pad(arr, padding_shape, mode='constant')\n",
    "            else:\n",
    "                padded_arr = arr\n",
    "            padded_image_features.append(padded_arr)\n",
    "\n",
    "        s = torch.from_numpy(np.concatenate(s, axis=0)).float()\n",
    "        a = torch.from_numpy(np.concatenate(a, axis=0)).float()\n",
    "        r = torch.from_numpy(np.concatenate(r, axis=0)).float()\n",
    "        rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).float()\n",
    "        timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).long()\n",
    "        mask = torch.from_numpy(np.concatenate(mask, axis=0)).float()\n",
    "        images = torch.from_numpy(np.concatenate(padded_image_features, axis=0)).float()\n",
    "        \n",
    "\n",
    "\n",
    "        return {\n",
    "            \"states\": s,\n",
    "            \"actions\": a,\n",
    "            \"rewards\": r,\n",
    "            \"returns_to_go\": rtg,\n",
    "            \"timesteps\": timesteps,\n",
    "            \"attention_mask\": mask,\n",
    "            \"image_features\": images,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainableDT(DecisionTransformerModel):\n",
    "    def __init__(self, config, image_feature_dim=768, gamma=0.99):\n",
    "        super().__init__(config)\n",
    "        self.gamma = gamma\n",
    "        self.image_feature_dim = image_feature_dim\n",
    "        self.state_projector = nn.Linear(config.state_dim + image_feature_dim, config.hidden_size)\n",
    "        self._states_projected = None\n",
    "\n",
    "    def embed_state(self, states):\n",
    "        # Use the projected states instead of embedding again\n",
    "        if self._states_projected is not None:\n",
    "            return self._states_projected\n",
    "        # Fallback to the default embed_state behavior if needed\n",
    "        return super().embed_state(states)\n",
    "\n",
    "    def forward(self, **kwargs): # kwarg need image features\n",
    "        # Reshape for custom loss\n",
    "        n_keys = 11\n",
    "        n_clicks = 2\n",
    "        mouse_x_possibles = [-1000.0,-500.0, -300.0, -200.0, -100.0, -60.0, -30.0, -20.0, -10.0, -4.0, -2.0, -0.0, 2.0, 4.0, 10.0, 20.0, 30.0, 60.0, 100.0, 200.0, 300.0, 500.0,1000.0]\n",
    "        mouse_y_possibles = [-200.0, -100.0, -50.0, -20.0, -10.0, -4.0, -2.0, -0.0, 2.0, 4.0, 10.0, 20.0, 50.0, 100.0, 200.0]\n",
    "        n_mouse_x = len(mouse_x_possibles)\n",
    "        n_mouse_y = len(mouse_y_possibles)\n",
    "        \n",
    "        # For image and state features\n",
    "        states = kwargs.pop(\"states\")\n",
    "        image_features = kwargs.pop(\"image_features\")\n",
    "        states_with_images = torch.cat([states, image_features], dim=-1)\n",
    "        # states_projected = self.state_projector(states_with_images)\n",
    "        self._states_projected = self.state_projector(states_with_images)\n",
    "        kwargs[\"states\"] = self._states_projected\n",
    "        output = super().forward(**kwargs)\n",
    "        self._states_projected = None\n",
    "        \n",
    "        action_preds = output[1]\n",
    "        action_targets = kwargs[\"actions\"]\n",
    "        attention_mask = kwargs[\"attention_mask\"]\n",
    "        act_dim = action_preds.shape[2]\n",
    "        action_preds = action_preds.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "        action_targets = action_targets.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "\n",
    "\n",
    "        # Action_pred n_keys: 11 keys for wasd, space, reload, 1,2,3\n",
    "        action_preds[:, :n_keys] = torch.sigmoid(action_preds[:, :n_keys])\n",
    "        # Action_pred n_clicks: left click, right click\n",
    "        action_preds[:, n_keys:n_keys+n_clicks] = torch.sigmoid(action_preds[:, n_keys:n_keys+n_clicks])\n",
    "        # Action_pred n_mouse_x: to the x mouse poll\n",
    "        action_preds[:, n_keys + n_clicks:n_keys + n_clicks + n_mouse_x] = F.softmax(action_preds[:, n_keys + n_clicks:n_keys + n_clicks + n_mouse_x], dim=-1)\n",
    "        # Action_pred n_mouse_y: to the y mouse poll\n",
    "        action_preds[:, n_keys + n_clicks + n_mouse_x:n_keys + n_clicks + n_mouse_x + n_mouse_y] = F.softmax(action_preds[:, n_keys + n_clicks + n_mouse_x:n_keys + n_clicks + n_mouse_x + n_mouse_y], dim=-1)\n",
    "\n",
    "        # Loss calculations based on each category\n",
    "        loss_wasd = F.binary_cross_entropy(action_preds[:, :4], action_targets[:, :4])\n",
    "        # loss_space = F.binary_cross_entropy(action_preds[:, 4:5], action_targets[:, 4:5]) # Temp removed for testing\n",
    "        # loss_reload = F.binary_cross_entropy(action_preds[:, n_keys-1:n_keys], action_targets[:, n_keys-1:n_keys]) # Temp removed for testing\n",
    "        # loss_weapon_switch = F.binary_cross_entropy(action_preds[:, n_keys-4:n_keys-1], action_targets[:, n_keys-4:n_keys-1]) # Temp removed for testing\n",
    "        loss_left_click = F.binary_cross_entropy(action_preds[:, n_keys:n_keys+1], action_targets[:, n_keys:n_keys+1])\n",
    "        # loss_right_click = F.binary_cross_entropy(action_preds[:, n_keys+1:n_keys+n_clicks], action_targets[:, n_keys+1:n_keys+n_clicks]) # Temp removed for testing\n",
    "        loss_mouse_move_x = F.cross_entropy(action_preds[:, n_keys+n_clicks:n_keys+n_clicks+n_mouse_x], action_targets[:, n_keys+n_clicks:n_keys+n_clicks+n_mouse_x])\n",
    "        loss_mouse_move_y = F.cross_entropy(action_preds[:, n_keys+n_clicks+n_mouse_x:n_keys+n_clicks+n_mouse_x+n_mouse_y], action_targets[:, n_keys+n_clicks+n_mouse_x:n_keys+n_clicks+n_mouse_x+n_mouse_y])\n",
    "        # Critic loss\n",
    "        # reward_t = action_targets[:, n_keys + n_clicks + n_mouse_x + n_mouse_y : n_keys + n_clicks + n_mouse_x + n_mouse_y + 1]\n",
    "        # v_t = action_preds[:, n_keys + n_clicks + n_mouse_x + n_mouse_y : n_keys + n_clicks + n_mouse_x + n_mouse_y + 1]\n",
    "        # v_t_next = action_preds[:, 1:, n_keys + n_clicks + n_mouse_x + n_mouse_y : n_keys + n_clicks + n_mouse_x + n_mouse_y + 1]\n",
    "\n",
    "        # loss_crit = 10 * F.mse_loss(reward_t[:, :-1] + self.gamma * v_t_next, v_t[:, :-1])\n",
    "        # Total loss\n",
    "        # total_loss = sum([3*loss_wasd,0.25*loss_space, 0.25*loss_reload,0.25*loss_weapon_switch, 2*loss_left_click, 0.25*loss_right_click, 2*loss_mouse_move_x,  2*loss_mouse_move_y])\n",
    "        total_loss = sum([3*loss_wasd, 2*loss_left_click, 2*loss_mouse_move_x,  2*loss_mouse_move_y])\n",
    "\n",
    "        # total_loss = sum([loss_wasd, loss_left_click, loss_mouse_move_x, loss_mouse_move_y])\n",
    "        print(\"loss\", total_loss)\n",
    "        return {\"loss\": total_loss}\n",
    "\n",
    "    def original_forward(self, **kwargs):\n",
    "        return super().forward(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom generator for streaming datasets\n",
    "def load_dataset_generator():\n",
    "    for folder in hf_folders:\n",
    "        yield load_from_disk(folder)\n",
    "\n",
    "\n",
    "main_data_path = \"d:\\\\test\"\n",
    "# Get all .hf folder paths\n",
    "hf_folders = [os.path.join(main_data_path, folder) for folder in os.listdir(main_data_path) if folder.endswith(\".hf\")]\n",
    "\n",
    "# Load dataset generator\n",
    "dataset_stream = load_dataset_generator()\n",
    "# for data in dataset_stream:\n",
    "#     print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ddfb50d4464b37a18bce95a1e7c1d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\setup_cac_kieu\\Anaconda\\New folder\\envs\\thesis\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:277: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  context_layer = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(15.6016, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(15.5664, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/trained_models/\",\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=32,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=1e-4,\n",
    "    warmup_ratio=0.3,\n",
    "    optim=\"adamw_torch\",\n",
    "    max_grad_norm=0.5,\n",
    ")\n",
    "# Initialize model and config once, outside the loop\n",
    "# Load the first dataset to get dimensions for the initial model configuration\n",
    "config = DecisionTransformerConfig(state_dim=61803, act_dim=51, hidden_size=1024, n_layer=20, n_head=16, dropout=0.0)\n",
    "first_dataset = next(dataset_stream)\n",
    "collator = DecisionTransformerGymDataCollator(first_dataset)\n",
    "model = TrainableDT(config)\n",
    "\n",
    "# First training session\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=first_dataset,\n",
    "    data_collator=collator\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# Save the initial model checkpoint\n",
    "save_path = \"./trained_models/model_after_dataset_1\"\n",
    "trainer.save_model(save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ++ model_after_dataset_1 ++ for training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9fc334e4f0348d5a563084bc242aee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(6.6169, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.7072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6140, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6248, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6022, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6076, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.5970, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.5630, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6716, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.5321, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "{'train_runtime': 1031.8245, 'train_samples_per_second': 0.194, 'train_steps_per_second': 0.01, 'train_loss': 6.613634490966797, 'epoch': 10.0}\n",
      "Model saved to ./trained_models/model_after_dataset_2\n",
      "Using ++ model_after_dataset_2 ++ for training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c83684c16540d2b443553bc3631b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(6.5791, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.7283, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.7035, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.7417, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.7717, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.5558, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6604, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.5830, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6124, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.7444, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "{'train_runtime': 959.1958, 'train_samples_per_second': 0.209, 'train_steps_per_second': 0.01, 'train_loss': 6.66802978515625, 'epoch': 10.0}\n",
      "Model saved to ./trained_models/model_after_dataset_3\n",
      "Using ++ model_after_dataset_3 ++ for training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c20e18473d4f8db0313a2485857434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(6.6867, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.7456, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.7543, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.7693, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.7293, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.7667, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.7124, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.7257, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6909, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.7433, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "{'train_runtime': 910.8756, 'train_samples_per_second': 0.22, 'train_steps_per_second': 0.011, 'train_loss': 6.732427978515625, 'epoch': 10.0}\n",
      "Model saved to ./trained_models/model_after_dataset_4\n",
      "Using ++ model_after_dataset_4 ++ for training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51826e52d6bb4edd83740ca7572b4ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(6.6419, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6133, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6117, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6170, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6227, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6269, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.5716, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.5815, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.5961, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6031, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "{'train_runtime': 925.6608, 'train_samples_per_second': 0.216, 'train_steps_per_second': 0.011, 'train_loss': 6.608587646484375, 'epoch': 10.0}\n",
      "Model saved to ./trained_models/model_after_dataset_5\n",
      "Using ++ model_after_dataset_5 ++ for training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41215ef16c664bf3b92119f287194ef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(6.6424, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.7032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6429, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6954, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6658, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6580, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6993, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6503, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.7187, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6597, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "{'train_runtime': 966.4653, 'train_samples_per_second': 0.207, 'train_steps_per_second': 0.01, 'train_loss': 6.673566436767578, 'epoch': 10.0}\n",
      "Model saved to ./trained_models/model_after_dataset_6\n",
      "Using ++ model_after_dataset_6 ++ for training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b663bafb6704e8bbc084f43df685270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(6.5472, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.4785, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.5512, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.4821, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.4853, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.5357, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.5700, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.5280, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.5029, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.5806, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "{'train_runtime': 831.868, 'train_samples_per_second': 0.24, 'train_steps_per_second': 0.012, 'train_loss': 6.526158142089844, 'epoch': 10.0}\n",
      "Model saved to ./trained_models/model_after_dataset_7\n",
      "Using ++ model_after_dataset_7 ++ for training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4a61b7fbe54525869e659b7712ce91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(6.4737, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.5162, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.4764, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.4955, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.4807, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.5199, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.5071, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.4336, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.5223, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.4868, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "{'train_runtime': 944.5032, 'train_samples_per_second': 0.212, 'train_steps_per_second': 0.011, 'train_loss': 6.4912055969238285, 'epoch': 10.0}\n",
      "Model saved to ./trained_models/model_after_dataset_8\n",
      "Using ++ model_after_dataset_8 ++ for training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74085ca5704a412a8c50fa4b2115e695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(6.5863, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.5529, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.5199, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.5570, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.5873, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.5561, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.5480, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.5927, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.5656, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.4702, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "{'train_runtime': 832.2365, 'train_samples_per_second': 0.24, 'train_steps_per_second': 0.012, 'train_loss': 6.553606414794922, 'epoch': 10.0}\n",
      "Model saved to ./trained_models/model_after_dataset_9\n",
      "Using ++ model_after_dataset_9 ++ for training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44457db678534ad78fca2829df67bf12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(6.5832, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6498, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.5516, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6462, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6111, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.5987, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.5831, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.5145, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.5971, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "{'train_runtime': 936.6848, 'train_samples_per_second': 0.214, 'train_steps_per_second': 0.011, 'train_loss': 6.594774627685547, 'epoch': 10.0}\n",
      "Model saved to ./trained_models/model_after_dataset_10\n",
      "Using ++ model_after_dataset_10 ++ for training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90463d0c45a54ef4aa080e32e0b953e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(6.6937, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6697, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6849, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6736, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.7287, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6369, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6157, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6649, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.7024, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6389, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "{'train_runtime': 835.576, 'train_samples_per_second': 0.239, 'train_steps_per_second': 0.012, 'train_loss': 6.670946502685547, 'epoch': 10.0}\n",
      "Model saved to ./trained_models/model_after_dataset_11\n",
      "Using ++ model_after_dataset_11 ++ for training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2517ba47288042f9b24f59be2d3ec453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(6.6534, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss tensor(6.6734, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 16\u001b[0m\n\u001b[0;32m      8\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m      9\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m!Theis\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mFinal_prj\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mtrained_models\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodel_after_dataset_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),  \u001b[38;5;66;03m# Use the model with weights updated from the previous training\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     11\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[0;32m     12\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mcollator\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Train on the current dataset\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Save the model after each training session\u001b[39;00m\n\u001b[0;32m     19\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./trained_models/model_after_dataset_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mf:\\setup_cac_kieu\\Anaconda\\New folder\\envs\\thesis\\lib\\site-packages\\transformers\\trainer.py:2122\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2120\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2123\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\setup_cac_kieu\\Anaconda\\New folder\\envs\\thesis\\lib\\site-packages\\transformers\\trainer.py:2426\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2424\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2425\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[1;32m-> 2426\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2427\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs \u001b[38;5;129;01min\u001b[39;00m batch_samples:\n\u001b[0;32m   2428\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mf:\\setup_cac_kieu\\Anaconda\\New folder\\envs\\thesis\\lib\\site-packages\\transformers\\trainer.py:5038\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[1;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[0;32m   5036\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[0;32m   5037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 5038\u001b[0m         batch_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m   5039\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   5040\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mf:\\setup_cac_kieu\\Anaconda\\New folder\\envs\\thesis\\lib\\site-packages\\accelerate\\data_loader.py:550\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 550\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[1;32mf:\\setup_cac_kieu\\Anaconda\\New folder\\envs\\thesis\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mf:\\setup_cac_kieu\\Anaconda\\New folder\\envs\\thesis\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mf:\\setup_cac_kieu\\Anaconda\\New folder\\envs\\thesis\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 69\u001b[0m, in \u001b[0;36mDecisionTransformerGymDataCollator.__call__\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     67\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m feature_extractor(images, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, do_rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     68\u001b[0m     vit_outputs \u001b[38;5;241m=\u001b[39m vit_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m---> 69\u001b[0m     image_features\u001b[38;5;241m.\u001b[39mappend(\u001b[43mvit_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_hidden_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vit_model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size)) \u001b[38;5;66;03m# need to get these thing to cpu to convert back to numpy, fitting other data\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# d.append(np.array(feature[\"dones\"][si : si + self.max_len]).reshape(1, -1))\u001b[39;00m\n\u001b[0;32m     72\u001b[0m timesteps\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marange(si, si \u001b[38;5;241m+\u001b[39m s[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Continue training with the remaining datasets\n",
    "for i, dataset in enumerate(dataset_stream, start=1):  # start=1 to continue from dataset 1 (meand second)\n",
    "    # Update collator with the new dataset\n",
    "    collator = DecisionTransformerGymDataCollator(dataset)\n",
    "\n",
    "    # Update the trainer with the current dataset and collator, but reuse the same model\n",
    "    print(f\"Using ++ model_after_dataset_{i} ++ for training\")\n",
    "    trainer = Trainer(\n",
    "        model=model.from_pretrained(f\"F:\\!Theis\\Final_prj\\\\trained_models\\model_after_dataset_{i}\"),  # Use the model with weights updated from the previous training\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        data_collator=collator\n",
    "    )\n",
    "\n",
    "    # Train on the current dataset\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the model after each training session\n",
    "    save_path = f\"./trained_models/model_after_dataset_{i+1}\"\n",
    "    trainer.save_model(save_path)\n",
    "    print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(f\"saved_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
